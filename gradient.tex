%
% Copyright © 2018 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%
%{
%%%\input{../latex/blogpost.tex}
%%%\renewcommand{\basename}{gradient}
%%%%\renewcommand{\dirname}{notes/phy1520/}
%%%\renewcommand{\dirname}{notes/ece1228-electromagnetic-theory/}
%%%%\newcommand{\dateintitle}{}
%%%%\newcommand{\keywords}{}
%%%
%%%\input{../latex/peeter_prologue_print2.tex}
%%%
%%%\usepackage{peeters_layout_exercise}
%%%\usepackage{peeters_braket}
%%%\usepackage{peeters_figures}
%%%\usepackage{siunitx}
%%%%\usepackage{mhchem} % \ce{}
%%%%\usepackage{macros_bm} % \bcM
%%%%\usepackage{macros_qed} % \qedmarker
%%%%\usepackage{txfonts} % \ointclockwise
%%%
%%%\beginArtNoToc
%%%
%%%\generatetitle{Gradient and vector derivative.}
%%%%\chapter{Gradient.}
%%%\label{chap:gradient}
%%%
%%%\paragraph{definition}
%%%\input{curvilinearThree.tex}
%%%
%%%\paragraph{Gradient.}
%%%
With the introduction of the ideas of reciprocal frame and curvilinear coordinates, we are getting closer to be able to formulate the geometric algebra generalizations of vector calculus.

The next step in the required mathematical preliminaries for geometric calculus is to determine the form of the gradient with respect to curvilinear coordinates and the
parameters associated with those coordinates.

Suppose we have a vector parameterization of \R{N}
\begin{equation}\label{eqn:gradient:60}
\Bx = \Bx(u_1, u_2, \cdots, u_N).
\end{equation}

We can employ the chain rule to express the gradient in terms of derivatives with respect to \( u_i \)
\begin{equation}\label{eqn:gradient:80}
\begin{aligned}
\spacegrad
&= \sum_i \Be_i \PD{x_i}{} \\
&= \sum_{i,j} \Be_i \PD{x_i}{u_j} \PD{u_j}{} \\
&= \sum_j \lr{ \sum_i \Be_i \PD{x_i}{u_j} } \PD{u_j}{} \\
&= \sum_j \lr{ \spacegrad u_j } \PD{u_j}{}.
\end{aligned}
\end{equation}

It turns out that the gradients of the parameters are in fact the reciprocal frame vectors

\input{Theorem_reciprocal_frame_vectors_gradient.tex}
\begin{proof}
This can be proven by direct computation
\begin{equation}\label{eqn:gradient:20}
\begin{aligned}
\Bx^i \cdot \Bx_j
&= (\spacegrad u_i) \cdot \PD{u_j}{\Bx} \\
&= \sum_{r,s=1}^n \lr{ \Be_r \PD{x_r}{u_i} } \cdot \lr{ \Be_s \PD{u_j}{x_s} } \\
&= \sum_{r,s=1}^n (\Be_r \cdot \Be_s) \PD{x_r}{u_i} \PD{u_j}{x_s} \\
&= \sum_{r,s=1}^n \delta_{rs} \PD{x_r}{u_i} \PD{u_j}{x_s} \\
&= \sum_{r=1}^n \PD{x_r}{u_i} \PD{u_j}{x_r} \\
&= \PD{u_i}{u_j} \\
&= \delta^i_j.
\end{aligned}
\end{equation}
This shows that \( \Bx^i = \spacegrad u_i \) has the properties required of the reciprocal frame, proving the theorem.
\end{proof}

We are now able to define the gradient with respect to an arbitrary set of parameters
\index{\(\spacegrad\)}
\index{\(\partial_i\)}
\input{Theorem_curvilinear_gradient.tex}

%}
%\EndArticle
