%
% Copyright Â© 2017 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%
\index{linear system}
\index{wedge product!linear solution}
%\maketheorem{Exact solution of a linear system.}{thm:solutionOfLinearSystem:1}{
%Given \( n \) linearly independent vectors \( \Ba_1, \Ba_2, \cdots \Ba_n \), the system
%\begin{equation*}
%\Ba_1 x_1 + \Ba_2 x_2 \cdots + \Ba_n x_n = \Bb,
%\end{equation*}
%if it has a solution, is given by
%\begin{equation*}
%\begin{aligned}
%x_1 &= i^{-1} \lr{ \Bb \wedge \Ba_2 \wedge \cdots \wedge \Ba_n } \\
%x_2 &= i^{-1} \lr{ \Ba_1 \wedge \Bb \wedge \cdots \wedge \Ba_n } \\
%    & \vdots \\
%x_n &= i^{-1} \lr{ \Ba_1 \wedge \Ba_2 \wedge \cdots \wedge \Bb },
%\end{aligned}
%\end{equation*}
%where
%\begin{equation*}
%i = \Ba_1 \wedge \Ba_2 \wedge \cdots \wedge \Ba_n,
%\end{equation*}
%is the pseudoscalar for the hypervolume spanned by \( \setlr{ \Ba_1, \cdots, \Ba_n } \).
%If the dimension of the vectors \( \Ba_i \) is \( n \), then this solution is equivalent to Cramer's rule.
%} % theorem
\maketheorem{Best fit solution of linear system.}{thm:solutionOfLinearSystem:2}{
Given \( k \) linearly independent vectors \( \Ba_1, \Ba_2, \cdots \Ba_k \), and the projection \( \Bb_\parallel \)
of a vector \( \Bb \) onto the hypervolume spanned by \( \setlr{ \Ba_1, \cdots, \Ba_k } \)
\begin{equation*}
\Bb_\parallel = i^{-1} \lr{ i \cdot \Bb },
\end{equation*}
where \( i = \Ba_1 \wedge \Ba_2 \wedge \cdots \wedge \Ba_k \), is a pseudoscalar for that hypervolume, then the system
\begin{equation*}
\Ba_1 x_1 + \Ba_2 x_2 \cdots + \Ba_k x_k = \Bb_\parallel,
\end{equation*}
is solved by
\begin{equation*}
\begin{aligned}
x_1 &= i^{-1} \cdot \lr{ \Bb \wedge \Ba_2 \wedge \cdots \wedge \Ba_k } \\
x_2 &= i^{-1} \cdot \lr{ \Ba_1 \wedge \Bb \wedge \cdots \wedge \Ba_k } \\
    & \vdots \\
x_n &= i^{-1} \cdot \lr{ \Ba_1 \wedge \Ba_2 \wedge \cdots \wedge \Bb }.
\end{aligned}
\end{equation*}

If \( \Bb \in \Span \setlr{ \Ba_1, \cdots, \Ba_k } \), so that \( \Bb_\parallel = \Bb \), then the dot products between the k-blades above may be dropped.
} % theorem
This is equivalent to a Moore-Penrose or SVD pseudoinverse solution for the system
\begin{equation}\label{eqn:solutionOfLinearSystem:101}
\begin{bmatrix}
\Ba_1 & \cdots & \Ba_n
\end{bmatrix}
\begin{bmatrix}
x_1 \\
\vdots \\
x_n
\end{bmatrix}
=
\Bb.
\end{equation}
Furthermore, also when the system is exact,
if the dimension of the vectors \( \Ba_i \) is \( k \), then this solution is equivalent to Cramer's rule.

Rather than formally trying to prove this theorem, we can tackle it informally, starting with some examples.
The simplest example is that of a two variable equation in \R{N}
\begin{equation}\label{eqn:solutionOfLinearSystem:1160}
\Ba x + \Bb y = \Bc.
\end{equation}
Let's proceed to solve this using the wedge product, assuming to start with that the system has an exact solution (i.e.: that \( \Bc \) is a linear combination of \( \Ba, \Bb \).)

To solve for \( x \) simply wedge with \( \Bb \), and to solve for \( y \) wedge with \( \Ba \)
\begin{equation}\label{eqn:solutionOfLinearSystem:1180}
\begin{aligned}
\lr{ \Ba x + \cancel{\Bb} y } \wedge \Bb &= \Bc \wedge \Bb \\
\Ba \wedge \lr{ \cancel{\Ba} x + \Bb y } &= \Ba \wedge \Bc,
\end{aligned}
\end{equation}
so, if the system has a solution, it is given by
\begin{equation}\label{eqn:solutionOfLinearSystem:1200}
\begin{aligned}
x &= \inv{ \Ba \wedge \Bb } \Bc \wedge \Bb \\
y &= \inv{ \Ba \wedge \Bb } \Ba \wedge \Bc.
\end{aligned}
\end{equation}

This idea generalizes trivially to
higher order systems can be solved, simply requiring wedging more times to eliminate all terms other than the one of interest.

For example, if the \( k \) variable system
\begin{equation}\label{eqn:solutionOfLinearSystem:1220}
\Ba_1 x_1 + \Ba_2 x_2 \cdots + \Ba_k x_k = \Bb,
\end{equation}
has a solution, we can solve for any of the \( x_i \)'s by wedging repeatedly.  For example, we can find \( x_1 \)
by wedging with all \( \Ba_2, \cdots \Ba_k \), to find
\begin{equation}\label{eqn:solutionOfLinearSystem:102}
x_1 \lr{ \Ba_1 \wedge \Ba_2 \wedge \cdots \wedge \Ba_k } = \Bb \wedge \Ba_2 \wedge \cdots \wedge \Ba_k,
\end{equation}
or
\begin{equation}\label{eqn:solutionOfLinearSystem:103}
x_1 = \inv{ \Ba_1 \wedge \Ba_2 \wedge \cdots \wedge \Ba_k } \lr{ \Bb \wedge \Ba_2 \wedge \cdots \wedge \Ba_k }
\end{equation}
If this system has no solution, then these k-vector ratios will not be scalars.

For the solution of \( x_k \), the numerator in each case is the denominator wedge product with the \( \Ba_k \) replaced by the solution vector \( \Bb \).
If this sounds like Cramer's rule, that is because the two are equivalent when the dimension of the vector equals the number of variables in the linear system.
\index{Cramer's rule}
For example, consider the solution for \( x_1 \) of \cref{eqn:solutionOfLinearSystem:1220} for an \R{3} system, with \( \Ba_1 = \Bu, \Ba_2 = \Bv, \Ba_3 = \Bw \)
\begin{equation}\label{eqn:solutionOfLinearSystem:1260}
x_1 =
\frac{ \Bb \wedge \Bv \wedge \Bw }
{ \Bu \wedge \Bv \wedge \Bw }
=
\frac{
\begin{vmatrix}
b_1 & v_1 & w_1 \\
b_2 & v_2 & w_2 \\
b_3 & v_3 & w_3 \\
\end{vmatrix}
\cancel{\Be_1 \Be_2 \Be_3}
}
{
\begin{vmatrix}
u_1 & v_1 & w_1 \\
u_2 & v_2 & w_2 \\
u_3 & v_3 & w_3 \\
\end{vmatrix}
\cancel{\Be_1 \Be_2 \Be_3}
},
\end{equation}
which is exactly the ratio of determinants found in the Cramer's rule solution of this problem.  We get Cramer's rule for free due to the antisymmetric structure of the wedge product.

Cramer's rule doesn't apply to cases where the dimension of the space exceeds the number of variables, but a wedge product solution does not have that restriction.  As an example, consider the two variable system \cref{eqn:solutionOfLinearSystem:1160} for vectors in \R{4} as follows
\begin{equation}\label{eqn:solutionOfLinearSystem:1280}
\Ba =
\begin{bmatrix}
1 \\
1 \\
0 \\
0
\end{bmatrix}, \qquad
\Bb =
\begin{bmatrix}
1 \\
0 \\
0 \\
1
\end{bmatrix}, \qquad
\Bc =
\begin{bmatrix}
1 \\
2 \\
0 \\
-1
\end{bmatrix}.
\end{equation}

%\iftoggle{kindle-version}{}{
Here's a (Mathematica) computation of the wedge products for the solution
%\href{https://github.com/jlaragonvera/Geometric-Algebra}{CliffordBasic.m}
\footnote{%
Using the CliffordBasic.m geometric algebra module from \citep{jlaragonveraGeometricAlgebra}.%
}

\begin{mmaCell}[moredefined={a, b, c, iab, aWedgeB, cWedgeB, aWedgeC, x, y, e, OuterProduct, GeometricProduct}]{Input}
  ClearAll[a, b, c, iab, aWedgeB, cWedgeB, aWedgeC, x, y]
  a = e[1] + e[2];
  b = e[1] + e[4];
  c = e[1] + 2 e[2] - e[4];

  aWedgeB = OuterProduct[a, b];
  cWedgeB = OuterProduct[c, b];
  aWedgeC = OuterProduct[a, c];

  (* 1/aWedgeB *)
  iab = aWedgeB / GeometricProduct[aWedgeB, aWedgeB];
  x = GeometricProduct[iab, cWedgeB];
  y = GeometricProduct[iab, aWedgeC];

  \{\{a \(\pmb{\wedge}\) b = , aWedgeB\},\{c \(\pmb{\wedge}\) b = , cWedgeB\},
  \{a \(\pmb{\wedge}\) c = , aWedgeC\},\{\"x = \", x\},\{\"y = \", y\}
  \} // Grid
\end{mmaCell}
\begin{mmaCell}{Output}
  a \(\wedge\) b = 	-e[1,2] + e[1,4] + e[2,4]
  c \(\wedge\) b = 	-2 e[1,2] + 2 e[1,4] + 2 e[2,4]
  a \(\wedge\) c = 	e[1,2] - e[1,4] - e[2,4]
  x = 	2
  y = 	-1
\end{mmaCell}

which shows that \( 2 \Ba - \Bb = \Bc \).
%}
%%Which gives
%%\begin{equation}\label{eqn:solutionOfLinearSystem:1300}
%%\begin{aligned}
%%\Ba \wedge \Bb &= \Be_{21} + \Be_{14} + \Be_{24} \\
%%\Bc \wedge \Bb &= 2 \Be_{21} + 2 \Be_{14} + 2 \Be_{24} \\
%%\Ba \wedge \Bc &= -\Be_{21} - \Be_{14} - \Be_{24} \\
%%x &= 2 \\
%%y &= -1.
%%\end{aligned}
%%\end{equation}
%%
\subsubsection{Example: intersection of two lines.}
As a concrete example, let's solve the intersection of two lines problem illustrated in \cref{fig:intersectionOfLines:intersectionOfLinesFig1}.
\pmathImageFigure{../figures/GAelectrodynamics/\subfigdir/}{intersectionOfLinesFig1}{Intersection of two lines.}{fig:intersectionOfLines:intersectionOfLinesFig1}{0.2}{orientedAreas.nb}

In parametric form, the lines in this problem are
\begin{equation}\label{eqn:solutionOfLinearSystem:1000}
\begin{aligned}
\Br_1(s) &= \Ba_0 + s( \Ba_1 - \Ba_0 ) \\
\Br_2(t) &= \Bb_0 + t( \Bb_1 - \Bb_0 ),
\end{aligned}
\end{equation}
so the solution, if it exists, is found at the point satisfying the equality
\begin{equation}\label{eqn:solutionOfLinearSystem:1020}
\Ba_0 + s( \Ba_1 - \Ba_0 ) = \Bb_0 + t( \Bb_1 - \Bb_0 ).
\end{equation}

With
\begin{equation}\label{eqn:solutionOfLinearSystem:1040}
\begin{aligned}
\Bu_1 &= \Ba_1 - \Ba_0 \\
\Bu_2 &= \Bb_1 - \Bb_0 \\
\Bd &= \Ba_0 - \Bb_0,
\end{aligned}
\end{equation}
the desired equation to solve is
\begin{equation}\label{eqn:solutionOfLinearSystem:1060}
\Bd + s \Bu_1 = t \Bu_2.
\end{equation}

As with any linear system, we can
solve for \( s \) or \( t \) by
wedging both sides with one of \( \Bu_1 \) or \( \Bu_2 \)
\begin{equation}\label{eqn:solutionOfLinearSystem:1080}
\begin{aligned}
\Bd \wedge \Bu_1 &= t \Bu_2 \wedge \Bu_1 \\
\Bd \wedge \Bu_2 + s \Bu_1 \wedge \Bu_2 &= 0.
\end{aligned}
\end{equation}

In \R{2} these equations have a solution if \( \Bu_1 \wedge \Bu_2 \ne 0 \), and in \R{N} these have solutions if the bivectors on each sides of the equations describe the same plane (i.e. the bivectors on each side of \cref{eqn:solutionOfLinearSystem:1080} are related by a scalar factor).
Put another way, these have solutions when \( s \) and \( t \) are scalars with the values
\begin{equation}\label{eqn:solutionOfLinearSystem:1100}
\begin{aligned}
s &= \frac{\Bu_2 \wedge \Bd}{\Bu_1 \wedge \Bu_2} \\
t &= \frac{\Bu_1 \wedge \Bd}{\Bu_1 \wedge \Bu_2}.
\end{aligned}
\end{equation}

%%%In
%%%\R{2}
%%%with
%%%\begin{equation}\label{eqn:solutionOfLinearSystem:1120}
%%%\begin{aligned}
%%%\Bu_1 &= u_{11} \Be_1 + u_{12} \Be_2 \\
%%%\Bu_2 &= u_{21} \Be_1 + u_{22} \Be_2 \\
%%%\Bd &= d_{1} \Be_1 + d_{2} \Be_2,
%%%\end{aligned}
%%%\end{equation}
%%%
%%%the wedge products in \cref{eqn:solutionOfLinearSystem:1100}
%%%can be expressed explicitly as a (unit bivector scaled) determinants
%%%
%%%\begin{equation}\label{eqn:solutionOfLinearSystem:1140}
%%%%\begin{aligned}
%%%s =
%%%\frac{
%%%\begin{vmatrix}
%%%u_{21} & u_{22} \\
%%%d_1 & d_2
%%%\end{vmatrix}
%%%\Be_{12}
%%%}
%%%{
%%%\begin{vmatrix}
%%%u_{11} & u_{12} \\
%%%u_{21} & u_{22} \\
%%%\end{vmatrix}
%%%\Be_{12}
%%%}
%%%%=
%%%%\frac{
%%%%\begin{vmatrix}
%%%%u_{21} & u_{22} \\
%%%%d_1 & d_2
%%%%\end{vmatrix}
%%%%}
%%%%{
%%%%\begin{vmatrix}
%%%%u_{11} & u_{12} \\
%%%%u_{21} & u_{22} \\
%%%%\end{vmatrix}
%%%%}
%%%\qquad
%%%t =
%%%\frac{
%%%\begin{vmatrix}
%%%u_{11} & u_{12} \\
%%%d_1 & d_2
%%%\end{vmatrix}
%%%\Be_{12}
%%%}
%%%{
%%%\begin{vmatrix}
%%%u_{11} & u_{12} \\
%%%u_{21} & u_{22} \\
%%%\end{vmatrix}
%%%\Be_{12}
%%%}
%%%%=
%%%%\frac{
%%%%\begin{vmatrix}
%%%%u_{11} & u_{12} \\
%%%%d_1 & d_2
%%%%\end{vmatrix}
%%%%}
%%%%{
%%%%\begin{vmatrix}
%%%%u_{11} & u_{12} \\
%%%%u_{21} & u_{22} \\
%%%%\end{vmatrix}
%%%%}
%%%.
%%%%\end{aligned}
%%%\end{equation}
%%%
%%%Once the unit bivectors \( \Be_{12} \) are cancelled \cref{eqn:solutionOfLinearSystem:1140} is the Cramer's rule solution of the problem.  Cramer's rule is seen to follow directly from the use of the wedge product to eliminate factors that are not of interest.
%%%In a similar way, the use of the wedge product for a 3D intersection problem with three variables, will lead directly to the Cramer's rule solution.
%%%
\makeproblem{Intersection of a line and plane.}{problem:solutionOfLinearSystem:1}{
Let a line be parameterized by
\begin{equation*}
\Br(a) = \Bp + \alpha \Ba,
\end{equation*}
and a plane be parameterized by
\begin{equation*}
\Br(b,c) = \Bq + \beta \Bb + \gamma \Bc.
\end{equation*}
\makesubproblem{}{problem:solutionOfLinearSystem:1:a}
For the intersection of the two, state the vector equation to be solved, and its solution for \( a \) in terms of a ratio of wedge products.
\makesubproblem{}{problem:solutionOfLinearSystem:1:b}
State the conditions for which the solution exist in \R{3} and \R{N}.
\makesubproblem{}{problem:solutionOfLinearSystem:1:c}
In terms of coordinates in \R{3} write out the ratio of wedge products as determinants and compare to the Cramer's rule solution.
} % problem
\makeanswer{problem:solutionOfLinearSystem:1}{
\makesubanswer{}{problem:solutionOfLinearSystem:1:a}
We are looking for solutions \( \alpha, \beta, \gamma \) such that the equality
\begin{equation}\label{eqn:solutionOfLinearSystem:1320}
\Bp + \alpha \Ba = \Bq + \beta \Bb + \gamma \Bc,
\end{equation}
is satisfied.
We have only to wedge with \( \Bb \wedge \Bc \), to find
\begin{equation}\label{eqn:solutionOfLinearSystem:1340}
\Bp \wedge \Bb \wedge \Bc + \alpha \lr{ \Ba \wedge \Bb \wedge \Bc }  = \Bq \wedge \Bb \wedge \Bc,
\end{equation}
or
\begin{equation}\label{eqn:solutionOfLinearSystem:1360}
\alpha = \frac{\lr{ \Bq - \Bp } \wedge \Bb \wedge \Bc }{ \Ba \wedge \Bb \wedge \Bc }.
\end{equation}
\makesubanswer{}{problem:solutionOfLinearSystem:1:b}
For \R{3}, a solution exists provided \( \Ba \wedge \Bb \wedge \Bc \ne 0 \), but for \R{N} a solution also requires
\begin{equation}\label{eqn:solutionOfLinearSystem:1380}
\lr{ \Bq - \Bp } \wedge \Bb \wedge \Bc \propto \Ba \wedge \Bb \wedge \Bc.
\end{equation}
For instance, there is no solution if \( \lr{ \Bq - \Bp } \wedge \Bb \wedge \Bc = \Be_{124} \), but \( \Ba \wedge \Bb \wedge \Bc = \Be_{234} \).
\makesubanswer{}{problem:solutionOfLinearSystem:1:c}
To solve this equation using coordinates, we seek solutions to
\begin{equation}\label{eqn:solutionOfLinearSystem:1400}
\Bp - \Bq = -\alpha \Ba + \beta \Bb + \gamma \Bc,
\end{equation}
or
\begin{equation}\label{eqn:solutionOfLinearSystem:1420}
\lr{ \Bp - \Bq } \cdot \Be_k = \lr{ -\alpha \Ba + \beta \Bb + \gamma \Bc } \cdot \Be_k,
\end{equation}
\( \forall k \in [1, N] \).  In matrix form, this is
\begin{equation}\label{eqn:solutionOfLinearSystem:1440}
\begin{bmatrix}
p_1 - q_1 \\
p_2 - q_2 \\
\vdots \\
p_N - q_N \\
\end{bmatrix}
=
\begin{bmatrix}
-a_1 & b_1 & c_1 \\
-a_2 & b_2 & c_2 \\
     & \vdots & \\
-a_N & b_N & c_N \\
\end{bmatrix}
\begin{bmatrix}
\alpha \\
\beta \\
\gamma
\end{bmatrix}.
\end{equation}
The Cramer's rule solution only applies to the \R{3} system, and has the form
\begin{equation}\label{eqn:solutionOfLinearSystem:1460}
\begin{bmatrix}
\alpha \\
\beta \\
\gamma
\end{bmatrix}
=
\frac{
\begin{vmatrix}
p_1 - q_1 & b_1 & c_1 \\
p_2 - q_2 & b_2 & c_2 \\
p_3 - q_3 & b_3 & c_3 \\
\end{vmatrix}
}
{
\begin{vmatrix}
-a_1 & b_1 & c_1 \\
-a_2 & b_2 & c_2 \\
-a_3 & b_3 & c_3 \\
\end{vmatrix}
}
=
\frac{
\begin{vmatrix}
q_1 - p_1 & b_1 & c_1 \\
q_2 - p_2 & b_2 & c_2 \\
q_3 - p_3 & b_3 & c_3 \\
\end{vmatrix}
}
{
\begin{vmatrix}
a_1 & b_1 & c_1 \\
a_2 & b_2 & c_2 \\
a_3 & b_3 & c_3 \\
\end{vmatrix}
}.
\end{equation}
This is obviously equivalent to the GA solution, where the ratio of determinants is found immediately from the coordinate representation of a triple wedge product.  We can't solve this system of equations using Cramer's rule for \R{N} when \( N > 3 \) since the system is overspecified in that case.  That overspecification is why we require the additional
\( \lr{ \Bq - \Bp } \wedge \Bb \wedge \Bc \propto \Ba \wedge \Bb \wedge \Bc \)
constraint for the GA solution using wedge products.  Note that this wedge product solution method is unlikely to be numerically stable for \( N > 3 \), and we are probably better off solving with SVD, so that we have some estimation of the numerical errors that either rule out or validate the solution.
} % answer
